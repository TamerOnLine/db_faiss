import os
import logging
import pandas as pd
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.agents import AgentType, initialize_agent, Tool
from langchain.memory import ConversationBufferMemory

# ØªÙ‡ÙŠØ¦Ø© logging Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ollama LLM
llm = OllamaLLM(model="llama3.2")

# ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡Ø§Øª Embeddings
embeddings = OllamaEmbeddings(model="locusai/multi-qa-minilm-l6-cos-v1")

# Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS
FAISS_PATH = "faiss_chat_index"

def initialize_faiss():
    """ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS."""
    if os.path.exists(os.path.join(FAISS_PATH, "index.faiss")):
        logging.info("ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
        return FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
    
    logging.info("âš ï¸ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§...")
    faiss_index = FAISS.from_texts(["Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!"], embeddings)
    faiss_index.save_local(FAISS_PATH)
    logging.info("âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    return faiss_index

faiss_index = initialize_faiss()

def search_faiss(query: str, k: int = 3, threshold: float = 0.9):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS Ø¹Ù† Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚ Ù„Ù„Ø·Ù„Ø¨."""
    try:
        results = faiss_index.similarity_search_with_score(query, k)
        for doc, score in results:
            if "â†’ Bot: " in doc.page_content and score >= threshold:
                stored_question, stored_answer = doc.page_content.split("â†’ Bot: ")
                if stored_question.strip().lower() == query.strip().lower():
                    return stored_answer
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ FAISS: {e}")
        return None
    
    return None

def store_chat(query: str, response: str):
    """ØªØ®Ø²ÙŠÙ† Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¯Ø§Ø®Ù„ FAISS."""
    if search_faiss(query, k=1, threshold=0.85):
        logging.warning("âš ï¸ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙÙŠ FAISSØŒ ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ®Ø²ÙŠÙ†.")
        return
    
    formatted_text = f"User: {query} â†’ Bot: {response}"
    faiss_index.add_texts([formatted_text])
    faiss_index.save_local(FAISS_PATH)
    logging.info("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¯Ø§Ø®Ù„ FAISS.")

def get_response(query: str):
    """Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¥Ù…Ø§ Ù…Ù† FAISS Ø£Ùˆ Ollama LLM."""
    response = search_faiss(query)
    
    if response is None:
        logging.info("ğŸ§  Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ FAISSØŒ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ollama...")
        response = llm.invoke(query)
        store_chat(query, response)
    
    return response

def load_data_from_file(file_path: str):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù CSV Ø£Ùˆ TXT Ø¥Ù„Ù‰ FAISS."""
    try:
        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path, encoding="utf-8")
            texts = df.iloc[:, 0].dropna().tolist()
        elif file_path.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as file:
                texts = [line.strip() for line in file.readlines() if line.strip()]
        else:
            logging.error("âš ï¸ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…! Ø§Ø³ØªØ®Ø¯Ù… CSV Ø£Ùˆ TXT.")
            return
        
        logging.info(f"ğŸ“¥ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ {len(texts)} Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ø¥Ù„Ù‰ FAISS...")
        faiss_index.add_texts(texts)
        faiss_index.save_local(FAISS_PATH)
        logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ FAISS.")
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {e}")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ
search_tool = Tool(
    name="FAISS Exact Search",
    func=search_faiss,
    description="ÙŠØ¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS Ø¹Ù† Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©."
)

llm_tool = Tool(
    name="Ollama LLM",
    func=llm.invoke,
    description="ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Ollama LLM Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª ÙÙŠ Ø­Ø§Ù„ Ø¹Ø¯Ù… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ FAISS."
)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

agent = initialize_agent(
    tools=[search_tool, llm_tool],
    llm=llm,
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    memory=memory
)

def chat_with_faiss():
    """ØªØ´ØºÙŠÙ„ Ø¬Ù„Ø³Ø© Ù…Ø­Ø§Ø¯Ø«Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS Ùˆ Ollama."""
    logging.info("ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø© Ø¹Ù„Ù‰ FAISS...")
    print("ğŸ”¹ Ø§ÙƒØªØ¨ 'exit' Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")
    
    while True:
        try:
            query = input("You: ").strip()
            if query.lower() == "exit":
                break
            
            response = agent.run(query)
            print(f"Bot: {response}")
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {e}")
            print("Ø­Ø¯Ø« Ø®Ø·Ø£ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

if __name__ == "__main__":
    chat_with_faiss()
